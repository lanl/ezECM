% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Bayes-ECM.R
\name{predict.BayesECM}
\alias{predict.BayesECM}
\title{New Event Categorization With Bayesian Inference}
\usage{
\method{predict}{BayesECM}(object, Ytilde, thinning = 1, mixture_weights = "training", ...)
}
\arguments{
\item{object}{an object of \code{class} \code{"BayesECM"} obtained as the trained model output from the \code{\link[=BayesECM]{BayesECM()}} function.}

\item{Ytilde}{\code{data.frame} of unlabeled observations to be categorized.  Must contain the same discriminant names as the training data used in the provided "BayesECM" object.  Each row is an individual event.  Missing data is specified with \code{NA}}

\item{thinning}{integer, scalar.  Values greater than one can be provided to reduce computation time.  See details.}

\item{mixture_weights}{character string describing the weights of the distributions in the mixture to be used for prediction.  The default,\code{"training"} will utilize weights according to likelihood and prior specifications, while supplying the string \code{"equal"} will assume the prior predictive category is independent of the training data and utilize equal weights.}

\item{...}{not used}
}
\value{
Returns a list of length \code{nrow(Ytilde)}, with each element of the list corresponding to an individual event provided by the \code{Ytilde} \code{data.frame}.  Each list element is a matrix, with each row of the matrix corresponding to an event category.  Each column is a length \eqn{K} sample of \eqn{p(\tilde{\mathbf{z}}_K|\tilde{\mathbf{y}}_{\tilde{p}}, \mathbf{Y}_{N \times p}, \mathbf{\theta})}, i.e. the probability of the event belonging to the row-specified event category.  See details for a further description.
}
\description{
New Event Categorization With Bayesian Inference
}
\details{
Before the data in \code{Ytilde} is used with the model, the p-values \eqn{\in (0,1]} must be transformed with the same function used on the training data to compute \code{object}.  The transform is automatically chosen to be the same as the \code{object} resulting from the trianing step.

For a given event with an unknown category, a Bayesian ECM model seeks to predict the distribution of latent variable \eqn{\mathbf{z}_K}, where \eqn{\mathbf{z}_K} is a vector of the length \eqn{K} and \eqn{K} is the number of event categories.  A single observation of \eqn{\mathbf{z}_K} contains a single value of 1, and is zero for the remaining entries, drawn from a \href{https://en.wikipedia.org/wiki/Categorical_distribution}{Categorical Distribution} conditioned on a probability for each category.  The index of the 1 corresponds to an event category.

The probabilities stipulated within the categorical distribution of \eqn{\mathbf{z}_K} are treated as unknown random variables inferred from the training data, prior hyperparameters, and independently each row of \code{Ytilde}. The output from \code{\link[=predict.BayesECM]{predict.BayesECM()}} are draws from the distribution of \eqn{\mathbb{E}[\tilde{\mathbf{z}}_K|\tilde{\mathbf{y}}_{\tilde{p}}, \mathbf{Y}_{N \times p}, \mathbf{\theta}] = p(\tilde{\mathbf{z}}_K|\tilde{\mathbf{y}}_{\tilde{p}}, \mathbf{Y}_{N \times p}, \mathbf{\theta})}.

The argument \code{mixture_weights} controls the value of \eqn{p(\tilde{\mathbf{z}}_K|\mathbf{Y}_{N \times p}, \mathbf{\theta})}, the distribution of \eqn{\tilde{\mathbf{z}}_K} before \eqn{\tilde{\mathbf{y}}_{\tilde{p}}} is observed.  Using \code{"training"} is best when the ratio of category observations is similar for the training data and the new observations \code{Ytilde}.  If the ratios of category observations are drastically different, it may be beneficial to use \code{"equal"}, which sets \eqn{p(\tilde{\mathbf{z}}_K| \mathbf{\theta}) = (1/K) \mathbf{1}^{\top}_K}.

To save computation time, the user can specify an integer value for \code{thinning} greater than one.  Every \code{thinning}th Markov-Chain Monte-Carlo sample gathered by the previous call to \code{\link[=BayesECM]{BayesECM()}} will be used within \code{\link[=predict.BayesECM]{predict.BayesECM()}}.  This lets the user take a large number of samples during the training step, allowing for better mixing, and then use less samples when categorizing unlabled data for a faster computation time with good mixing.
}
\examples{

csv_use <- "good_training.csv"
file_path <- system.file("extdata", csv_use, package = "ezECM")
training_data <- import_pvals(file = file_path, header = TRUE, sep = ",", training = TRUE)

trained_model <- BayesECM(Y = training_data, BT = c(10,1000))

csv_use <- "good_newdata.csv"
file_path <- system.file("extdata", csv_use, package = "ezECM")
new_data <- import_pvals(file = file_path, header = TRUE, sep = ",", training = TRUE)

Zsamples <- predict(trained_model,  Ytilde = new_data)


}
